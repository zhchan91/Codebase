---
title: "STA 243 Assignment 5"
author: "Chen Zihao(50%) Man Pan(50%)"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,fig.height=3)
```

```{r}
n = 100
x = seq(0,1,length= (n+1))[-(n+1)]+0.5/n

phi = function(x) exp((-x^2)/2)/sqrt(2*pi)
f = function(x) 1.5*phi((x-0.35)/0.15)-phi((x-0.8)/0.04)

#using model assumption
ma = function(j){f(x) + (0.02+0.04*(3-1)^2)*rnorm(n,0,1)}
```


We fix the number of knots as 30, and place them equi-spaced within the domain of the data. Let $x_{(1)} = \min(x_i)$ and $x_{(n)} = \max(x_i)$ so that we have the location of the $k$th knot,$t_k = x_{(1)}+ k\frac {x_{(n)}-x_{(1)}}{31}$.

Equi-spaced knots may cause a issue that there may be no data within two knots. In this homework it will not happen because $\{x_i\}$ is a arithmetic series. In this problem, it is equivalent to take all the unique x out and then choose the quantile points which will ensure all the subspace have at least data if the number of knots is appropriate.

the design matrix is as below.
$$
X =\left[
\begin{matrix}
1 & X_1 & X_1^2 & X_1^3 & (X_1-t_1)_+^3 & (X_1-t_2)_+^3 & ... & (X_1-t_{30})_+^3\\
1 & X_2 & X_2^2 & X_2^3 & (X_2-t_1)_+^3 & (X_2-t_2)_+^3 & ... & (X_2-t_{30})_+^3\\
... & ... & ... & ... & ... & ... & ...& ... \\
1 & X_n & X_n^2 & X_n^3 & (X_n-t_1)_+^3 & (X_n-t_2)_+^3 & ... & (X_n-t_{30})_+^3\\
\end{matrix}
\right]
$$
we can get
$$
\hat f=X\hat\beta=H_{\lambda}Y = X(X^\top X+\lambda D)^{-1}X^\top Y
$$
where $D = diag(0,0,0,0,1,1,...,1)$ is a $34\times34$ matrix.

```{r}

p <- 3
num.knots <- 30

# build the design matrix

design <- function(x){
  knots <- quantile(unique(x), seq(0, 1, length = num.knots + 2))[-c(1, (num.knots + 2))]
  xpoly <- rep(1, n)
  for (j in 1:p) xpoly <- cbind(xpoly, x^j)
  xspline <- outer(x, knots, "-")
  xspline <- pmax(xspline, 0)^p
  X <- cbind(xpoly, xspline)
}
X <- design(x)

# penalty matrix
D <- diag(1+num.knots+p)
for (i in 1:(1+p)){D[i,i]=0}

#H_lambda matrix
H_lambda = function(X, D, lambda){
  tmp <- solve( t(X) %*% X + lambda * D, t(X)) #(XTX)^-1 XT
  H <- X %*% tmp
  return(H)
}

#get the predict value
predict.ls = function(y, X, D, lam){
  # get y_hat and residuals
  H <- H_lambda( X, D, lam)
  yhat.ls <- H %*% y
  residuals <-  y - yhat.ls
  return(list(pred=yhat.ls,resid=residuals))
}
```




### (a) Cross- validation (CV)

$$
\begin{split}
\sum_{i=1}^n(y_i-\hat f_{-i})^2 \approx \sum_{i=1}^n(\frac {y_i - \hat f_i}{1-h_{ii}})^2
\end{split}
$$
where $\{h_ii\}$ is the diagonal elements of $H_{\lambda}$
```{r}
cv <- function(y, X, D, lambda) 
{ 
  # y is the response vector
  # X is the big design matrix
  # D is the penalty matrix
  # lambda is the value of the penalty constant to be evaluated
  
  H <- H_lambda( X, D, lambda)
  y_hat <- H %*% y
  r <- (y - y_hat)
  return( sum( r^2  / (1 - diag(H))^2 ))
}

pen.ls.cv <- function(y, X, D,accurate = 30) {
  #find the best lambda by this criterion
  lam = 4
  criter = c()
  for (i in -2:accurate){
    tem = seq(max(0,lam-2^-i),lam+2^-i,length.out = 5)[-c(1,5)]
    for (j in 1:3) criter[j] = cv(y,X,D,tem[j])
    lam = tem[criter == min(criter)][1]
  }
  return(lam)
}
```

We use $y_i = 15\phi(\frac{x_i-0.35}{0.15})-10\phi(\frac{x_i-0.8}{0.04})+\epsilon_i$ similar to the paper to test our algorithm

```{r}
set.seed(125)
y = ma()
lambda = pen.ls.cv(y,X,D)
sig_cv = cv(y,X,D,lambda)/(n-1-p-num.knots) # save this for later use. to estimate the sigma^2.
plot(x,predict.ls(y,X,D,lambda)$pred,type="l",xlab = "",ylab = "",main = paste("CV:the best lambda is",lambda))
par(new=TRUE)
plot(x,y,axes = FALSE,xlab = "", ylab = "")
```

### Generalized CV (GCV)

replace $h_{ii}$ by the average of the diagonal elements of $H_\lambda$
$$
GCV = \sum_{i=1}^n(\frac {y_i - \hat f_i}{1-\frac {tr(H_\lambda)}{n}})^2
$$

```{r}
gcv <- function(y, X, D, lambda) 
{ 
  # y is the response vector
  # X is the big design matrix
  # D is the penalty matrix
  # lambda is the value of the penalty constant to be evaluated
  H <- H_lambda(X, D, lambda)
  y_hat <- H %*% y
  r <- (y - y_hat)
  return( n * sum( r^2 ) / (n - sum(diag(H)))^2 )
}

pen.ls.gcv <- function(y, X, D,accurate = 30) {
  #find the best lambda by this criterion
  lam = 4
  criter = c()
  for (i in -2:accurate){
    tem = seq(max(0,lam-2^-i),lam+2^-i,length.out =5)[-c(1,5)]
    for (j in 1:3) criter[j] = gcv(y,X,D,tem[j])
    lam = tem[criter == min(criter)][1]
  }
  return(lam)
}

```

We use the same data as previous one, to test our algorithm

```{r}
lambda = pen.ls.gcv(y,X,D)
plot(x,predict.ls(y,X,D,lambda)$pred,type="l",xlab = "",ylab = "",main = paste("GCV:the best lambda is",lambda))
par(new=TRUE)
plot(x,y,axes = FALSE,xlab = "", ylab = "")
```



### (b) AICC

```{r}
aicc = function(y, X, D, lambda) {
  n = length(y)
  H <- H_lambda(X, D, lambda)
  y_hat <- H %*% y
  r <- (y - y_hat)
  tr = sum(diag(H))
  return(log(sum( r^2 ))+ 2*(tr+1)/(n-tr-2))
}

pen.ls.aicc <- function(y, X, D,accurate = 30) {
  #find the best lambda by this criterion
  lam = 4
  criter = c()
  for (i in -2:accurate){
    tem = seq(max(0,lam-2^-i),lam+2^-i,length.out = 5)[-c(1,5)]
    for (j in 1:3) criter[j] = aicc(y,X,D,tem[j])
    lam = tem[criter == min(criter)][1]
  }
  return(lam)
}
```
We use the same data to test our algorithm

```{r}
lambda = pen.ls.aicc(y,X,D)
plot(x,predict.ls(y,X,D,lambda)$pred,type="l",xlab = "",ylab = "",main = paste("AICC:the best lambda is",lambda))
par(new=TRUE)
plot(x,y,axes = FALSE,xlab = "", ylab = "")
```



### (c) Estimate the expectation of the risk($\lambda$)

$y_i = f(x_i) +\epsilon_{i}$

$$
\begin{split}
E||y-\hat f_\lambda||^2 &= E||f+e-H_\lambda(f+e)||^2\\
& = E||(I-H_\lambda)(f+e)||^2\\
& = E(((I-H_\lambda)(f+e))^\top((I-H_\lambda)(f+e)))\\
& = E((f+e)^\top(I-H_\lambda)^\top(I-H_\lambda)(f+e))\\
& = ||(I-H_\lambda)f||^2+E(f^\top(I-H_\lambda)^\top(I-H_\lambda)e)+E(e^\top(I-H_\lambda)^\top(I-H_\lambda)f)+E(e^\top(I-H_\lambda)^\top(I-H_\lambda)e)\\
& = ||(I-H_\lambda)f||^2+E(e^\top(I-H_\lambda)^\top(I-H_\lambda)e)\\
& = ||(I-H_\lambda)f||^2+E(e^\top(I-H_\lambda-H_\lambda^\top+H_\lambda^\top H_\lambda)e)\\
& = ||(I-H_\lambda)f||^2+E(tr(e^\top(I-2H_\lambda+H_\lambda^\top H_\lambda)e))\\
& = ||(I-H_\lambda)f||^2+E(tr((I-2H_\lambda+H_\lambda^\top H_\lambda)ee^\top))\\
& = ||(I-H_\lambda)f||^2+tr(E((I-2H_\lambda+H_\lambda^\top H_\lambda)ee^\top))\\
& = ||(I-H_\lambda)f||^2+\sigma^2 (tr(n-2tr(H_\lambda)+tr(H_\lambda^\top H_\lambda))\\
& = ||(I-H_\lambda)f||^2+\sigma^2 \{tr(tr(H_\lambda H_\lambda^\top)-2tr(H_\lambda)+n\}\\
\end{split}
$$
now consider $E||f-\hat f_\lambda||^2$
$$
\begin{split}
E||f-\hat f_\lambda||^2 &= E||f-y+y-H_\lambda y||^2\\
& = E||f-y||^2 + 2E(f-y)^\top(y-H_\lambda y) + E||y-H_\lambda y||^2\\
& = n\sigma^2 - 2E\epsilon^\top(I-H_\lambda)y+E||y-H_\lambda y||^2\\
& = n\sigma^2 - 2E\epsilon^\top(I-H_\lambda)\epsilon+E||y-H_\lambda y||^2\\
& = n\sigma^2 - 2tr(I-H_\lambda)\sigma^2+||y-H_\lambda y||^2\\
& = E||y-\hat f_\lambda||^2+(2tr(H_\lambda)-n)\sigma^2\\
\end{split}
$$
We use the folowing to estimate the $E||f-\hat f_\lambda||^2$
$$
||y-\hat f_\lambda||^2+(2tr(H_\lambda)-n)\hat\sigma^2
$$
still need to estimate $\hat \sigma^2$,CV is a good estimate so we will use the result from CV.


```{r}
#expect_risk
er = function(y, X, D, lambda,sigma) {
  n = length(y)
  H <- H_lambda(X, D, lambda)
  y_hat <- H %*% y
  r <- (y - y_hat)
  tr = sum(diag(H))
  return( sum( r^2 )+ 2*(tr-n)*sigma)
}

pen.ls.er <- function(y, X, D,sigma,accurate = 30) {
  #find the best lambda by this criterion
  lam = 4
  criter = c()
  for (i in -2:accurate){
    tem = seq(max(0,lam-2^-i),lam+2^-i,length.out = 5)[-c(1,5)]
    for (j in 1:3) criter[j] = er(y,X,D,tem[j],sigma)
    lam = tem[criter == min(criter)][1]
  }
  return(lam)
}
```

We use the same data to test our algorithm

```{r}
#sigma = sig_cv as already calculate in part (a)
lambda = pen.ls.er(y,X,D,sig_cv)
plot(x,predict.ls(y,X,D,lambda)$pred,type="l",xlab = "",ylab = "",main = paste("ER:the best lambda is",lambda))
par(new=TRUE)
plot(x,y,axes = FALSE,xlab = "", ylab = "")
```

### (d) using the simulation setting to test.

$\log r$,defined as below, is used to measure the performance of the 4 criteria. We simulate 100 times to plot boxplots to do the visulization. 

$$
\log r = \log \frac{||f-\hat f_\lambda||^2}{\min_\lambda ||f-\hat f_\lambda||^2}
$$

For the denominator, we first use binary search to find the minimizer. Then, we use the $\lambda$ given by the 4 criteria to do a search in order to make sure our binary search did not traped at the local minimum. Although we can not make sure we get the global minimizer, the $\lambda$ is still good enough to analyze. According to our algorithm, the $\lambda$ in the denominator is more acurrate than the $\lambda$'s given by the criteria.  

### Noise level

$$
y_{ij} = f(x_i)+\sigma_j\epsilon_i\\
\sigma_j=0.02+0.04(j-1)^2
$$

```{r}
#Noise level
nl = function(j){f(x) + (0.02+0.04*(j-1)^2)*rnorm(n,0,1)}
```



```{r}
res = function(y,x,X,D,lam){
  sum((f(x)-H_lambda(X,D,lam) %*% y)^2)
}
Denominator = function (y,x,X,D,lam_4){
  lam = 1
  criter = c()
  for (i in -1:15){
    tem = seq(max(0,lam-10^-i),lam+10^-i,length.out = 21)[-c(1,21)]
    for (j in 1:19) criter[j] = res(y,x,X,D,tem[j])
    lam = tem[criter == min(criter)][1]
  }
  #it might get the local minimizer
  
  tem2 = c(seq(min(lam_4)/2,max(lam_4)*2,length.out = 10),lam_4)
  criter2 = c()
  for (i in 1:14){criter2[i] = res(y,x,X,D,tem2[i])}
  
  
  return(min(c(criter,criter2)))
}
```



```{r}
erlist = data.frame()
aicclist = data.frame()
cvlist = data.frame()
gcvlist = data.frame()
for (j in 1:6){
  for (i in 1:100){
    y = nl(j)
    aicc_lam = pen.ls.aicc(y,X,D)
    gcv_lam = pen.ls.gcv(y,X,D)
    cv_lam = pen.ls.cv(y,X,D)
    
    sig_cv = cv(y,X,D,cv_lam)/(n-1-p-num.knots)
    er_lam = pen.ls.er(y,X,D,sig_cv)
    
    lam_4 = c(aicc_lam,gcv_lam,cv_lam,er_lam)
    deno = Denominator(y,x,X,D,lam_4)
    
    aicclist[i,j] = log(res(y,x,X,D,aicc_lam)/deno)
    gcvlist[i,j] = log(res(y,x,X,D,gcv_lam)/deno)
    cvlist[i,j] = log(res(y,x,X,D,cv_lam)/deno)
    erlist[i,j] = log(res(y,x,X,D,er_lam)/deno)
  }
}
```

```{r,fig.width=7}
for (i in 1:6){
  y=nl(i)
  par(mfrow=c(1,2))
  plot(x,y,cex=0.5,ylim = c(-0.4-0.4*i,0.4+0.4*i))
  par(new=TRUE)
  plot(x,f(x),type="l",ylab = "",xlab = "",main = "Noise level",ylim = c(-0.4-0.4*i,0.4+0.4*i))
  
  data = data.frame(aicc = aicclist[,i],gcv = gcvlist[,i],cv = cvlist[,i],er = erlist[,i])
  boxplot(data,main=paste("j =",i))
}
```

### Design Density

$$
y_{ij}=f(X_{ji})+\sigma\epsilon_i\\
\sigma=0.1,X_{ji}=F^{-1}(X_i)
$$
where F_j is the Beta$(\frac{j+4}{5},\frac{11-j}{5})$

```{r}
#Design density
dd = function(j){
  x = qbeta(x,(j+4)/5,(11-j)/5)
  y = f(x)+0.1*rnorm(n,0,1)
  return(list(x=x,y=y))
} 
```

```{r}
erlist = data.frame()
aicclist = data.frame()
cvlist = data.frame()
gcvlist = data.frame()
for (j in 1:6){
  for (i in 1:100){
    x = sort(runif(n,0,1))
    xy = dd(j)
    x = xy$x
    X = design(x)
    y = xy$y
    aicc_lam = pen.ls.aicc(y,X,D)
    gcv_lam = pen.ls.gcv(y,X,D)
    cv_lam = pen.ls.cv(y,X,D)
    
    sig_cv = cv(y,X,D,cv_lam)/(n-1-p-num.knots)
    er_lam = pen.ls.er(y,X,D,sig_cv)
    
    lam_4 = c(aicc_lam,gcv_lam,cv_lam,er_lam)
    deno = Denominator(y,x,X,D,lam_4)
    
    aicclist[i,j] = log(res(y,x,X,D,aicc_lam)/deno)
    gcvlist[i,j] = log(res(y,x,X,D,gcv_lam)/deno)
    cvlist[i,j] = log(res(y,x,X,D,cv_lam)/deno)
    erlist[i,j] = log(res(y,x,X,D,er_lam)/deno)
  }
}
```


```{r,fig.width=7}
for (i in 1:6){
  x = sort(runif(n,0,1))
  xy <-  dd(i)
  x <- xy$x
  y <- xy$y
  
  par(mfrow=c(1,2))
  plot(x,y,cex=0.5,ylim = c(-0.8,1),xlim=c(0,1))
  par(new=TRUE)
  plot(x,f(x),type="l",ylab = "",xlab = "",main = "Design Density",ylim = c(-0.8,1),xlim=c(0,1))
  
  data = data.frame(aicc = aicclist[,i],gcv = gcvlist[,i],cv = cvlist[,i],er = erlist[,i])
  boxplot(data,main=paste("j =",i))
}
```

### Spatial variation

```{r}
x = seq(0,1,length= (n+1))[-(n+1)]+0.5/n
X = design(x)
#Spatial variation
fj = function(j){
  m1 = 2^((9-4*j)/5)
  m2 = 2*pi*(1+m1)
  sqrt(x*(1-x))*sin(m2/(x+m1))
}
```

```{r}
#these two function will change according to the change of the function.
res_sv = function(y,X,D,lam){
  sum((fj_y-H_lambda(X,D,lam) %*% y)^2)
}

Denominator_sv = function (y,x,X,D,lam_4){
  lam = 1
  criter = c()
  for (i in -1:15){
    tem = seq(max(0,lam-10^-i),lam+10^-i,length.out = 21)[-c(1,21)]
    for (j in 1:19) criter[j] = res_sv(y,X,D,tem[j])
    lam = tem[criter == min(criter)][1]
  }
  #it might get the local minimizer
  
  tem2 = c(seq(min(lam_4)/2,max(lam_4)*2,length.out = 10),lam_4)
  criter2 = c()
  for (i in 1:14){criter2[i] = res_sv(y,X,D,tem2[i])}
  
  return(min(criter,criter2))
}
```

```{r,fig.width=7}
erlist = data.frame()
aicclist = data.frame()
cvlist = data.frame()
gcvlist = data.frame()


for (j in 1:6){
  fj_y = fj(j)
  for (i in 1:100){
    y = fj_y +0.2*rnorm(n,0,1)
    aicc_lam = pen.ls.aicc(y,X,D)
    gcv_lam = pen.ls.gcv(y,X,D)
    cv_lam = pen.ls.cv(y,X,D)
    
    sig_cv = cv(y,X,D,cv_lam)/(n-1-p-num.knots)
    er_lam = pen.ls.er(y,X,D,sig_cv)

    lam_4 = c(aicc_lam,gcv_lam,cv_lam,er_lam)
    deno = Denominator_sv(y,x,X,D,lam_4)
    
    aicclist[i,j] = log(res_sv(y,X,D,aicc_lam)/deno)
    gcvlist[i,j] = log(res_sv(y,X,D,gcv_lam)/deno)
    cvlist[i,j] = log(res_sv(y,X,D,cv_lam)/deno)
    erlist[i,j] = log(res_sv(y,X,D,er_lam)/deno)
  }
}


for (i in 1:6){
  fj_y = fj(i)
  y = fj_y +0.2*rnorm(n,0,1)
  par(mfrow=c(1,2))
  plot(x,y,cex=0.5,ylim = c(-0.7,0.7),main = "Spatial variation")
  par(new=TRUE)
  plot(x,fj_y,type="l",ylab = "",xlab = "",ylim = c(-0.7,0.7))
  
  data = data.frame(aicc = aicclist[,i],gcv = gcvlist[,i],cv = cvlist[,i],er = erlist[,i])
  boxplot(data,main=paste("j =",i))
}

```

### Variance function
```{r}
#Variance function
vf = function(j){
  vj = (0.15*(1 + 0.4*(2*j-7)*(x-0.5)))^2
  f(x) + sqrt(vj)*rnorm(n,0,1)
}
```


```{r}
erlist = data.frame()
aicclist = data.frame()
cvlist = data.frame()
gcvlist = data.frame()
for (j in 1:6){
  for (i in 1:100){
    y = vf(j)
    aicc_lam = pen.ls.aicc(y,X,D)
    gcv_lam = pen.ls.gcv(y,X,D)
    cv_lam = pen.ls.cv(y,X,D)
    
    sig_cv = cv(y,X,D,cv_lam)/(n-1-p-num.knots)
    er_lam = pen.ls.er(y,X,D,sig_cv)
    
    lam_4 = c(aicc_lam,gcv_lam,cv_lam,er_lam)
    deno = Denominator(y,x,X,D,lam_4)
    
    aicclist[i,j] = log(res(y,x,X,D,aicc_lam)/deno)
    gcvlist[i,j] = log(res(y,x,X,D,gcv_lam)/deno)
    cvlist[i,j] = log(res(y,x,X,D,cv_lam)/deno)
    erlist[i,j] = log(res(y,x,X,D,er_lam)/deno)
  }
}
```

```{r,fig.width=7}
for (i in 1:6){
  y=vf(i)
  par(mfrow=c(1,2))
  plot(x,y,cex=0.5,ylim = c(-1,1))
  par(new=TRUE)
  plot(x,f(x),type="l",ylab = "",xlab = "",main = "Variance function",ylim = c(-1,1))
  
  data = data.frame(aicc = aicclist[,i],gcv = gcvlist[,i],cv = cvlist[,i],er = erlist[,i])
  boxplot(data,main=paste("j =",i))
}
```


### Conclusion:

In each pair plot, the left one presents one typical simulated data set together with the true function, the right one is the boxplot of $\log r$.
From the plot, we can find there is not a best criterion to choose the smoothing parameter, because the performance of those 4 criteria are so similar. Specially, CV and GCV have very similar results.