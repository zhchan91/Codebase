{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chen Zihao 915490404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write the gradient descent algorithm with fixed step size for solving (1).\n",
    "\n",
    "$$\n",
    "w^* = \\arg\\min_w\\{\\frac 1 n \\sum_{i=1}^n(x_i^Tw-y_i)^2+\\frac\\lambda 2 ||w||^2\\} := f(w) \\eqno{(1)}\n",
    "$$\n",
    "\n",
    "f(w) can be also writen as\n",
    "\n",
    "$$\n",
    "f(w) = \\frac 1 n (y-Xw)^T(y-Xw) +\\frac\\lambda 2 w^Tw  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\triangledown f = -\\frac 2 n (X^Ty)  + (\\frac 2 n X^TX+\\lambda  I) w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-0.035365</td>\n",
       "      <td>-0.006296</td>\n",
       "      <td>-0.032874</td>\n",
       "      <td>-0.036778</td>\n",
       "      <td>-0.024605</td>\n",
       "      <td>-0.137983</td>\n",
       "      <td>-0.071508</td>\n",
       "      <td>-0.066856</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>-0.320106</td>\n",
       "      <td>-0.006862</td>\n",
       "      <td>0.302082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>-0.026040</td>\n",
       "      <td>-0.017017</td>\n",
       "      <td>-0.049686</td>\n",
       "      <td>-0.050686</td>\n",
       "      <td>-0.038322</td>\n",
       "      <td>-0.077378</td>\n",
       "      <td>-0.060021</td>\n",
       "      <td>-0.061121</td>\n",
       "      <td>-0.053329</td>\n",
       "      <td>-0.170709</td>\n",
       "      <td>0.042530</td>\n",
       "      <td>0.165858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>-0.005172</td>\n",
       "      <td>-0.004010</td>\n",
       "      <td>-0.011641</td>\n",
       "      <td>-0.011950</td>\n",
       "      <td>-0.009727</td>\n",
       "      <td>-0.013480</td>\n",
       "      <td>-0.010702</td>\n",
       "      <td>-0.012064</td>\n",
       "      <td>-0.010593</td>\n",
       "      <td>-0.024124</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.025637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.001077</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>-0.001145</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.001146</td>\n",
       "      <td>-0.002508</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.002698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0  1.000000e-02 -0.035365 -0.006296 -0.032874 -0.036778 -0.024605 -0.137983   \n",
       "1  1.000000e-03 -0.026040 -0.017017 -0.049686 -0.050686 -0.038322 -0.077378   \n",
       "2  1.000000e-04 -0.005172 -0.004010 -0.011641 -0.011950 -0.009727 -0.013480   \n",
       "3  1.000000e-05 -0.000560 -0.000440 -0.001279 -0.001315 -0.001077 -0.001442   \n",
       "4  1.000000e-06 -0.000057 -0.000044 -0.000129 -0.000133 -0.000109 -0.000145   \n",
       "5  1.000000e-07 -0.000006 -0.000004 -0.000013 -0.000013 -0.000011 -0.000015   \n",
       "\n",
       "         7         8         9         10        11        12  \n",
       "0 -0.071508 -0.066856 -0.054443 -0.320106 -0.006862  0.302082  \n",
       "1 -0.060021 -0.061121 -0.053329 -0.170709  0.042530  0.165858  \n",
       "2 -0.010702 -0.012064 -0.010593 -0.024124  0.009798  0.025637  \n",
       "3 -0.001145 -0.001305 -0.001146 -0.002508  0.001071  0.002698  \n",
       "4 -0.000115 -0.000132 -0.000116 -0.000252  0.000108  0.000271  \n",
       "5 -0.000012 -0.000013 -0.000012 -0.000025  0.000011  0.000027  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "filename = \"./cpusmall\"\n",
    "X,Y = datasets.load_svmlight_file(filename)\n",
    "X_array = sparse.csr_matrix.todense(X)\n",
    "\n",
    "#standardized X and y to get rid of the NaN issue.\n",
    "x = scale(X_array)\n",
    "y = scale(Y)\n",
    "    \n",
    "def GDFSS(eta,X,y):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Fixed Step Size\n",
    "    \"\"\"\n",
    "    #set w0\n",
    "    p = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    epsilon = 0.001\n",
    "    w = np.zeros(p)\n",
    "        \n",
    "    #for every iteration we need to calculate 2X^TX/n+I \n",
    "    #and -2/n*X^Ty, store it to speed up.\n",
    "    XTX = X.T.dot(X)*2/n+np.eye(p)\n",
    "    XY = -2/n*X.T.dot(y)\n",
    "    \n",
    "    def f1(w):\n",
    "        return XY+XTX.dot(w)\n",
    "    \n",
    "    g = f1(w)\n",
    "    r0 = np.linalg.norm(g)\n",
    "    for i in range(200):\n",
    "        if np.linalg.norm(g)<epsilon*r0:\n",
    "            break \n",
    "        g = f1(w)\n",
    "        w = w - eta*g\n",
    "    return w\n",
    "\n",
    "result=np.zeros([6,13])\n",
    "for i in range(2,8):\n",
    "    result[i-2,0] = 10**-i\n",
    "    result[i-2,1:] = GDFSS(10**-i,x,y)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the first collumns is the $\\eta$ and the remain of each row is the $w$ after 200 iterations.\n",
    "\n",
    "I compared it as the result of 20000 iterations to see the stability. It seems the $10^{-2}$ is the best which is already around the stable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Do 5-fold cross validation. Each time using 4 of them as training and 1 of them as testing to get MSE, and then report the average MSE.\n",
    "\n",
    "$$\n",
    "MSE = \\frac 1 n \\sum_{i=1}^n (x_i^Tw-y_i)^2 = \\frac 1 n (Xw-y)^T(Xw-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(w,X,y):\n",
    "    n = X.shape[0]\n",
    "    e = X.dot(w)-y\n",
    "    return e.T.dot(e)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33792027939007935"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x = pd.DataFrame(x)\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "# get the number of rows\n",
    "n = X_array.shape[0]\n",
    "\n",
    "#to get a random list\n",
    "t = np.arange(n)\n",
    "random.shuffle(t)\n",
    "\n",
    "MSE_sum = 0\n",
    "\n",
    "for i in range(5):\n",
    "    #get the validation\n",
    "    test = t[i*round(n/5):(i+1)*round(n/5)]\n",
    "    x_test = x.iloc[test,]\n",
    "    y_test = y.iloc[test]\n",
    "    x_train = x.drop(test)\n",
    "    y_train = y.drop(test)\n",
    "    #to get the w\n",
    "    w = GDFSS(10**-2,x_train,y_train)\n",
    "    #get the MSE\n",
    "    MSE_sum = MSE_sum + MSE(w,x_test,y_test).iloc[0,0]\n",
    "MSE_sum/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run gradient descent on \"E2006-tfidf\" data. Run your gradient descent implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"./E2006.train.bz2\"\n",
    "X_train,Y_train = datasets.load_svmlight_file(filename)\n",
    "filename = \"./E2006.test.bz2\"\n",
    "X_test,Y_test = datasets.load_svmlight_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a very large sparse matrix, i need to modify the algorithm in 1.1 to make it quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\triangledown f = -\\frac 2 n (X^Ty)  + \\frac 2 n X^TXw+\\lambda  w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GDFSS2(eta,X,y):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Fixed Step Size\n",
    "    \"\"\"\n",
    "    #set w0\n",
    "    p = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    epsilon = 0.001\n",
    "    w = np.zeros(p)\n",
    "        \n",
    "    #for every iteration we need to calculate -2/n*X^Ty\n",
    "    #store it to speed up.\n",
    "    XY = -2*X.T@(y/n)\n",
    "    \n",
    "    def f1(w):\n",
    "        return XY+2*X.T@(X@(w/n))+w\n",
    "    \n",
    "    g = f1(w)\n",
    "    r0 = np.linalg.norm(g)\n",
    "    for i in range(200):\n",
    "        if np.linalg.norm(g)<epsilon*r0:\n",
    "            break \n",
    "        g = f1(w)\n",
    "        w = w - eta*g\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15098585179963231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MSE(w,X,y):\n",
    "    n = X.shape[0]\n",
    "    e = X@w - y\n",
    "    return e.T@e/n\n",
    "\n",
    "w = GDFSS2(5*10**-2,X_train,Y_train)\n",
    "MSE(w[:-2],X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w^* = \\arg\\min_w \\{\\frac 1 n \\sum_{i=1}^n \\log (1+e^{-y_iw^Tx_i})+\\frac \\lambda 2 ||w||^2\\}:= f(w) \\eqno(2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Derive the gradient of (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\triangledown f(w) &=\\frac 1 n \\sum_{i=1}^n \\frac {1} {1+e^{-y_iw^Tx_i}}\\times e^{-y_iw^Tx_i}\\times (-y_ix_i)+\\lambda w\\\\\n",
    "&= -\\frac 1 n \\sum_{i=1}^n \\frac {y_ix_i}{1+e^{y_iw^Tx_i}}+\\lambda w\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a matrix form.\n",
    "$$\n",
    "\\triangledown f(w)= -\\frac 1 n \\frac{X^Ty}{1+e^{W^TX^Ty}}+\\lambda w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Implement gradient descent with fixed step size to solve (2). Split it into 80% training and 20% testing. Solve the logistic regression probelm using $\\lambda=1$ on the training set, and report the prediction accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"./news20.binary.bz2\"\n",
    "X,Y = datasets.load_svmlight_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "#to get a random list\n",
    "t = np.arange(n)\n",
    "random.shuffle(t)\n",
    "\n",
    "Y_train,X_train = Y[t[:round(0.8*n)]],X[t[:round(0.8*n)],]\n",
    "Y_test,X_test = Y[t[round(0.8*n):]],X[t[round(0.8*n):],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def Logi(eta,X,y):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Fixed Step Size \n",
    "    \"\"\"\n",
    "    #set w0\n",
    "    p = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    epsilon = 0.001\n",
    "    w = np.zeros(p)\n",
    "        \n",
    "    #for every iteration we need to calculate -2/n*X^Ty\n",
    "    #store it to speed up.\n",
    "    XY = -2*X.T@(y/n)\n",
    "    \n",
    "    def f1(w,x,y):\n",
    "        return -x.T@y/((1+math.exp(w.T@x.T@y))*x.shape[0])+w\n",
    "    \n",
    "    g = f1(w,X,y)\n",
    "    r0 = np.linalg.norm(g)\n",
    "    for i in range(200):\n",
    "        if np.linalg.norm(g)<epsilon*r0:\n",
    "            break \n",
    "        g = f1(w,X,y)\n",
    "        w = w - eta*g\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = Logi(10**-2,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    s = x@w\n",
    "    n =s.shape[0]\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in range(n):\n",
    "        y[i] = 1/(1+math.exp(-s[i]))\n",
    "    y = np.sign(y-0.5+10**-15)#to get rid of those exact 0.5\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the confusion matrix for logistic regression and we can use it to calcurate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1831,  656],\n",
       "       [ 202, 1310]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(predict(w,X_test),Y_test)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion[1,0]+confusion[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854463615903976"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(confusion[0,0]+confusion[1,1])/sum(sum(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that I got about 850 wrong and i got about 79% accuracy.(change as the the split change)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
